{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import explainability as exp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm Class: 0.7\n",
      "Model Size Score: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\Desktop\\UNI\\L_IACD\\3_Year\\2_Semester\\INV\\Code\\INV_2024\\explainability.py:81: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgorithm Class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;241m.\u001b[39malgorithm_class_score(reg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Size Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;241m.\u001b[39mmodel_size_score(reg,\u001b[38;5;250m \u001b[39mX)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelated Features Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;241m.\u001b[39mcorrelated_features_score(X)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Importance Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;241m.\u001b[39mfeature_importance_score(reg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShap Coefficient of variance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;241m.\u001b[39mcv_shap_score(reg,\u001b[38;5;250m \u001b[39mX)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anton\\Desktop\\UNI\\L_IACD\\3_Year\\2_Semester\\INV\\Code\\INV_2024\\explainability.py:81\u001b[0m, in \u001b[0;36mcorrelated_features_score\u001b[1;34m(dataset, target_column, verbose)\u001b[0m\n\u001b[0;32m     78\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mcorr()\u001b[38;5;241m.\u001b[39mabs()\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Select upper triangle of correlation matrix\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m upper \u001b[38;5;241m=\u001b[39m corr_matrix\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39mtriu(np\u001b[38;5;241m.\u001b[39mones(corr_matrix\u001b[38;5;241m.\u001b[39mshape), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m))\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Compute average and standar deviation from upper correlation matrix \u001b[39;00m\n\u001b[0;32m     84\u001b[0m avg_corr \u001b[38;5;241m=\u001b[39m upper\u001b[38;5;241m.\u001b[39mvalues[np\u001b[38;5;241m.\u001b[39mtriu_indices_from(upper\u001b[38;5;241m.\u001b[39mvalues,\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\numpy\\__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=10, n_targets=1, random_state=123)\n",
    "X = pd.DataFrame(X)\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(X,y)\n",
    "\n",
    "print(f\"Algorithm Class: {exp.algorithm_class_score(reg)}\")\n",
    "print(f\"Model Size Score: {exp.model_size_score(reg, X)}\")\n",
    "\n",
    "print(f\"Correlated Features Score: {exp.correlated_features_score(X)}\")\n",
    "print(f\"Feature Importance Score: {exp.feature_importance_score(reg)}\")\n",
    "print(f\"Shap Coefficient of variance: {exp.cv_shap_score(reg, X)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm Class: 0.8\n",
      "Model Size Score: 9\n",
      "Correlated Features Score: 0.5555555555555556\n",
      "Feature Importance Score: 0.4\n",
      "Shap Coefficient of variance: 0.6969347183053155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10,flip_y=0, n_redundant=0, n_repeated=0, n_clusters_per_class=2, n_classes=5, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "clf = RandomForestClassifier(random_state=123)\n",
    "\n",
    "clf.fit(X,y)\n",
    "\n",
    "\n",
    "print(f\"Algorithm Class: {exp.algorithm_class_score(clf)}\")\n",
    "print(f\"Model Size Score: {exp.model_size_score(clf, X)}\")\n",
    "\n",
    "print(f\"Correlated Features Score: {exp.correlated_features_score(X)}\")\n",
    "print(f\"Feature Importance Score: {exp.feature_importance_score(clf)}\")\n",
    "print(f\"Shap Coefficient of variance: {exp.cv_shap_score(clf, X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m X, y \u001b[38;5;241m=\u001b[39m make_classification(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, n_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_informative\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,flip_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_redundant\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_repeated\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_clusters_per_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X)\n\u001b[1;32m----> 7\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \n\u001b[0;32m     10\u001b[0m TFmodel \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     11\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     12\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m     13\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m ])\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10,flip_y=0, n_redundant=0, n_repeated=0, n_clusters_per_class=2, n_classes=5, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1)) \n",
    "\n",
    "TFmodel = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "TFmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "TFmodel.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "print(f\"Algorithm Class: {exp.algorithm_class_score(TFmodel)}\")\n",
    "print(f\"Model Size Score: {exp.model_size_score(TFmodel, X)}\")\n",
    "\n",
    "print(f\"Correlated Features Score: {exp.correlated_features_score(X)}\")\n",
    "print(f\"Feature Importance Score: {exp.feature_importance_score(TFmodel)}\")\n",
    "print(f\"Shap Coefficient of variance: {exp.cv_shap_score(TFmodel, X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# One-hot encode the labels\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m y_onehot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encoder\u001b[38;5;241m.\u001b[39mfit_transform(y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Create a dataset and dataloaders\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=10, flip_y=0, n_redundant=0, \n",
    "    n_repeated=0, n_clusters_per_class=2, n_classes=5, random_state=42\n",
    ")\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = torch.tensor(encoder.fit_transform(y.reshape(-1, 1)), dtype=torch.float32)\n",
    "\n",
    "# Create a dataset and dataloaders\n",
    "dataset = TensorDataset(X, y_onehot)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the model\n",
    "class TorchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.dense1 = nn.Linear(10, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(32, 5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(self.dense2(x))\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        # x is numpy not tensor, return is numpy\n",
    "        xx = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs = torch.exp(self.forward(xx))\n",
    "        return probs.numpy()\n",
    "\n",
    "# Create an instance of the model\n",
    "Tmodel = TorchModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Tmodel.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    Tmodel.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = Tmodel(inputs)\n",
    "        loss = criterion(outputs, labels.argmax(dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / train_size\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Validation loop\n",
    "    Tmodel.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = Tmodel(inputs)\n",
    "            loss = criterion(outputs, labels.argmax(dim=1))\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
    "    \n",
    "    val_loss /= val_size\n",
    "    accuracy = correct / val_size\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "print(f\"Algorithm Class: {exp.algorithm_class_score(Tmodel)}\")\n",
    "print(f\"Model Size Score: {exp.model_size_score(X)}\")\n",
    "\n",
    "print(f\"Correlated Features Score: {exp.correlated_features_score(X)}\")\n",
    "print(f\"Feature Importance Score: {exp.feature_importance_score(Tmodel)}\")\n",
    "#print(f\"Shap Coefficient of variance: {exp.cv_shap_score(Tmodel, X)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unknown type passed as data object: <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m      2\u001b[0m background \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39msample(X, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKernelExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\shap\\explainers\\_kernel.py:96\u001b[0m, in \u001b[0;36mKernelExplainer.__init__\u001b[1;34m(self, model, data, feature_names, link, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index_ordered \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_index_ordered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m convert_to_model(model, keep_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index)\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m model_null \u001b[38;5;241m=\u001b[39m match_model_to_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# enforce our current input type limitations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\shap\\utils\\_legacy.py:230\u001b[0m, in \u001b[0;36mconvert_to_data\u001b[1;34m(val, keep_index)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparseData(val)\n\u001b[0;32m    229\u001b[0m emsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown type passed as data object: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(val)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(emsg)\n",
      "\u001b[1;31mTypeError\u001b[0m: Unknown type passed as data object: <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "background = shap.sample(X, 100)\n",
    "explainer = shap.KernelExplainer(Tmodel, background)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unknown type passed as data object: <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShap Coefficient of variance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;241m.\u001b[39mcv_shap_score(Tmodel,\u001b[38;5;250m \u001b[39mX)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anton\\Desktop\\UNI\\L_IACD\\3_Year\\2_Semester\\INV\\Code\\INV_2024\\explainability.py:162\u001b[0m, in \u001b[0;36mcv_shap_score\u001b[1;34m(clf, test_data)\u001b[0m\n\u001b[0;32m    159\u001b[0m background \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39msample(test_data, \u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Using 100 samples for background\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Initialize KernelExplainer with the prediction function and background dataset\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKernelExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Calculate SHAP values for the dataset you want to explain\u001b[39;00m\n\u001b[0;32m    165\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(test_data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\shap\\explainers\\_kernel.py:96\u001b[0m, in \u001b[0;36mKernelExplainer.__init__\u001b[1;34m(self, model, data, feature_names, link, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index_ordered \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_index_ordered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m convert_to_model(model, keep_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index)\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m model_null \u001b[38;5;241m=\u001b[39m match_model_to_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# enforce our current input type limitations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\shap\\utils\\_legacy.py:230\u001b[0m, in \u001b[0;36mconvert_to_data\u001b[1;34m(val, keep_index)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparseData(val)\n\u001b[0;32m    229\u001b[0m emsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown type passed as data object: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(val)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(emsg)\n",
      "\u001b[1;31mTypeError\u001b[0m: Unknown type passed as data object: <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "print(f\"Shap Coefficient of variance: {exp.cv_shap_score(Tmodel, X)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
