{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import explainability as exp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm Class: 3.5\n",
      "Model Size Score: 5\n",
      "Correlated Features Score: 1\n",
      "Feature Importance Score: 5\n",
      "Shap Coefficient of variance: 0.9597673377706508\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=10, n_targets=1, random_state=123)\n",
    "X = pd.DataFrame(X)\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(X,y)\n",
    "\n",
    "print(f\"Algorithm Class: {exp.algorithm_class_score(reg)}\")\n",
    "print(f\"Model Size Score: {exp.model_size_score(X)}\")\n",
    "\n",
    "print(f\"Correlated Features Score: {exp.correlated_features_score(X)}\")\n",
    "print(f\"Feature Importance Score: {exp.feature_importance_score(reg)}\")\n",
    "print(f\"Shap Coefficient of variance: {exp.cv_shap_score(reg, X)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm Class: 4\n",
      "Model Size Score: 5\n",
      "Correlated Features Score: 1\n",
      "Feature Importance Score: 5\n",
      "Shap Coefficient of variance: 0.6969347183053155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10,flip_y=0, n_redundant=0, n_repeated=0, n_clusters_per_class=2, n_classes=5, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "clf = RandomForestClassifier(random_state=123)\n",
    "\n",
    "clf.fit(X,y)\n",
    "\n",
    "\n",
    "print(f\"Algorithm Class: {exp.algorithm_class_score(clf)}\")\n",
    "print(f\"Model Size Score: {exp.model_size_score(X)}\")\n",
    "\n",
    "print(f\"Correlated Features Score: {exp.correlated_features_score(X)}\")\n",
    "print(f\"Feature Importance Score: {exp.feature_importance_score(clf)}\")\n",
    "print(f\"Shap Coefficient of variance: {exp.cv_shap_score(clf, X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.2003 - loss: 2.5791 - val_accuracy: 0.2100 - val_loss: 2.0454\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2411 - loss: 2.2748 - val_accuracy: 0.2450 - val_loss: 1.8294\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2604 - loss: 2.1646 - val_accuracy: 0.2900 - val_loss: 1.6669\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3254 - loss: 1.8466 - val_accuracy: 0.3400 - val_loss: 1.5418\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3232 - loss: 1.7992 - val_accuracy: 0.3650 - val_loss: 1.4365\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3442 - loss: 1.7696 - val_accuracy: 0.4000 - val_loss: 1.3466\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3856 - loss: 1.5023 - val_accuracy: 0.4500 - val_loss: 1.2797\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3925 - loss: 1.5207 - val_accuracy: 0.4850 - val_loss: 1.2271\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4446 - loss: 1.4025 - val_accuracy: 0.5250 - val_loss: 1.1868\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4003 - loss: 1.3863 - val_accuracy: 0.5300 - val_loss: 1.1457\n",
      "Algorithm Class: 1\n",
      "Model Size Score: 5\n",
      "Correlated Features Score: 1\n",
      "Feature Importance Score: None\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m3194/3194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n",
      "Shap Coefficient of variance: 0.7621601751500174\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10,flip_y=0, n_redundant=0, n_repeated=0, n_clusters_per_class=2, n_classes=5, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "TFmodel = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "TFmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "TFmodel.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "print(f\"Algorithm Class: {exp.algorithm_class_score(TFmodel)}\")\n",
    "print(f\"Model Size Score: {exp.model_size_score(X)}\")\n",
    "\n",
    "print(f\"Correlated Features Score: {exp.correlated_features_score(X)}\")\n",
    "print(f\"Feature Importance Score: {exp.feature_importance_score(TFmodel)}\")\n",
    "print(f\"Shap Coefficient of variance: {exp.cv_shap_score(TFmodel, X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.6192\n",
      "Validation Loss: 1.6016, Accuracy: 0.2350\n",
      "Epoch 2/10, Loss: 1.5911\n",
      "Validation Loss: 1.5799, Accuracy: 0.3450\n",
      "Epoch 3/10, Loss: 1.5637\n",
      "Validation Loss: 1.5609, Accuracy: 0.4050\n",
      "Epoch 4/10, Loss: 1.5495\n",
      "Validation Loss: 1.5412, Accuracy: 0.4100\n",
      "Epoch 5/10, Loss: 1.5177\n",
      "Validation Loss: 1.5221, Accuracy: 0.4300\n",
      "Epoch 6/10, Loss: 1.4979\n",
      "Validation Loss: 1.5030, Accuracy: 0.4500\n",
      "Epoch 7/10, Loss: 1.4766\n",
      "Validation Loss: 1.4873, Accuracy: 0.4600\n",
      "Epoch 8/10, Loss: 1.4647\n",
      "Validation Loss: 1.4718, Accuracy: 0.4700\n",
      "Epoch 9/10, Loss: 1.4362\n",
      "Validation Loss: 1.4576, Accuracy: 0.4700\n",
      "Epoch 10/10, Loss: 1.4228\n",
      "Validation Loss: 1.4448, Accuracy: 0.4650\n",
      "Algorithm Class: 1\n",
      "Model Size Score: 5\n",
      "Correlated Features Score: 1\n",
      "Feature Importance Score: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=10, flip_y=0, n_redundant=0, \n",
    "    n_repeated=0, n_clusters_per_class=2, n_classes=5, random_state=42\n",
    ")\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = torch.tensor(encoder.fit_transform(y.reshape(-1, 1)), dtype=torch.float32)\n",
    "\n",
    "# Create a dataset and dataloaders\n",
    "dataset = TensorDataset(X, y_onehot)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the model\n",
    "class TorchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.dense1 = nn.Linear(10, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(32, 5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(self.dense2(x))\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        # x is numpy not tensor, return is numpy\n",
    "        xx = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs = torch.exp(self.forward(xx))\n",
    "        return probs.numpy()\n",
    "\n",
    "# Create an instance of the model\n",
    "Tmodel = TorchModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Tmodel.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    Tmodel.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = Tmodel(inputs)\n",
    "        loss = criterion(outputs, labels.argmax(dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / train_size\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Validation loop\n",
    "    Tmodel.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = Tmodel(inputs)\n",
    "            loss = criterion(outputs, labels.argmax(dim=1))\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
    "    \n",
    "    val_loss /= val_size\n",
    "    accuracy = correct / val_size\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "print(f\"Algorithm Class: {exp.algorithm_class_score(Tmodel)}\")\n",
    "print(f\"Model Size Score: {exp.model_size_score(X)}\")\n",
    "\n",
    "print(f\"Correlated Features Score: {exp.correlated_features_score(X)}\")\n",
    "print(f\"Feature Importance Score: {exp.feature_importance_score(Tmodel)}\")\n",
    "#print(f\"Shap Coefficient of variance: {exp.cv_shap_score(Tmodel, X)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unknown type passed as data object: <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m      2\u001b[0m background \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39msample(X, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKernelExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\shap\\explainers\\_kernel.py:96\u001b[0m, in \u001b[0;36mKernelExplainer.__init__\u001b[1;34m(self, model, data, feature_names, link, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index_ordered \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_index_ordered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m convert_to_model(model, keep_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index)\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m model_null \u001b[38;5;241m=\u001b[39m match_model_to_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# enforce our current input type limitations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\shap\\utils\\_legacy.py:230\u001b[0m, in \u001b[0;36mconvert_to_data\u001b[1;34m(val, keep_index)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparseData(val)\n\u001b[0;32m    229\u001b[0m emsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown type passed as data object: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(val)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(emsg)\n",
      "\u001b[1;31mTypeError\u001b[0m: Unknown type passed as data object: <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "background = shap.sample(X, 100)\n",
    "explainer = shap.KernelExplainer(Tmodel, background)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unknown type passed as data object: <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShap Coefficient of variance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;241m.\u001b[39mcv_shap_score(Tmodel,\u001b[38;5;250m \u001b[39mX)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anton\\Desktop\\UNI\\L_IACD\\3_Year\\2_Semester\\INV\\Code\\INV_2024\\explainability.py:162\u001b[0m, in \u001b[0;36mcv_shap_score\u001b[1;34m(clf, test_data)\u001b[0m\n\u001b[0;32m    159\u001b[0m background \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39msample(test_data, \u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Using 100 samples for background\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Initialize KernelExplainer with the prediction function and background dataset\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKernelExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Calculate SHAP values for the dataset you want to explain\u001b[39;00m\n\u001b[0;32m    165\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(test_data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\shap\\explainers\\_kernel.py:96\u001b[0m, in \u001b[0;36mKernelExplainer.__init__\u001b[1;34m(self, model, data, feature_names, link, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index_ordered \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_index_ordered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m convert_to_model(model, keep_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index)\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m model_null \u001b[38;5;241m=\u001b[39m match_model_to_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# enforce our current input type limitations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\shap\\utils\\_legacy.py:230\u001b[0m, in \u001b[0;36mconvert_to_data\u001b[1;34m(val, keep_index)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparseData(val)\n\u001b[0;32m    229\u001b[0m emsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown type passed as data object: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(val)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(emsg)\n",
      "\u001b[1;31mTypeError\u001b[0m: Unknown type passed as data object: <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "print(f\"Shap Coefficient of variance: {exp.cv_shap_score(Tmodel, X)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
