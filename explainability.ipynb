{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Class Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "\n",
    "def algorithm_class_score(clf):\n",
    "\n",
    "    alg_score = {\n",
    "    \"RandomForestClassifier\": 4,\n",
    "    \"KNeighborsClassifier\": 3,\n",
    "    \"SVC\": 2,\n",
    "    \"GaussianProcessClassifier\": 3,\n",
    "    \"DecisionTreeClassifier\": 5,\n",
    "    \"MLPClassifier\": 1,\n",
    "    \"AdaBoostClassifier\": 3,\n",
    "    \"GaussianNB\": 3.5,\n",
    "    \"QuadraticDiscriminantAnalysis\": 3,\n",
    "    \"LogisticRegression\": 4,\n",
    "    \"LinearRegression\": 3.5,\n",
    "    }\n",
    "\n",
    "    clf_name = type(clf).__name__\n",
    "\n",
    "    # Check if the clf_name is in the dictionary\n",
    "    if clf_name in alg_score:\n",
    "        exp_score = alg_score[clf_name]\n",
    "        return exp_score \n",
    "\n",
    "    # Check if the model is a Neural Network\n",
    "    if isinstance(clf, tf.keras.Model) or isinstance(clf, tf.Module) or isinstance(clf, nn.Module):\n",
    "        return 1\n",
    "    \n",
    "    # If not, try to find a close match\n",
    "    close_matches = get_close_matches(clf_name, alg_score.keys(), n=1, cutoff=0.6)\n",
    "    if close_matches:\n",
    "        exp_score = alg_score[close_matches[0]]\n",
    "        return exp_score\n",
    "    \n",
    "    # If no close match found \n",
    "    print(f\"No matching score found for '{clf_name}'\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "5\n",
      "DecisionTreeRegressor\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Example Decision Tree Classifer and Regressor\n",
    "from sklearn import tree\n",
    "\n",
    "Classifier = tree.DecisionTreeClassifier()\n",
    "Regressor = tree.DecisionTreeRegressor()\n",
    "\n",
    "print(type(Classifier).__name__)\n",
    "print(algorithm_class_score(Classifier))\n",
    "\n",
    "print(type(Regressor).__name__)\n",
    "print(algorithm_class_score(Regressor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Example Neural Network Tensorflow \n",
    "import tensorflow as tf\n",
    "\n",
    "TFNN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, input_dim=128, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(type(TFNN).__name__)\n",
    "print(algorithm_class_score(TFNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Custom non-sequential NN using keras\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.dense2(x)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "print(type(model).__name__)\n",
    "print(algorithm_class_score(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Example Neural Network Pytoch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 64)  \n",
    "        self.fc2 = nn.Linear(64, 32)  \n",
    "        self.fc3 = nn.Linear(32, 1)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))   \n",
    "        x = torch.relu(self.fc2(x))    \n",
    "        x = torch.sigmoid(self.fc3(x)) \n",
    "        return x\n",
    "    \n",
    "TOCHNN = NeuralNetwork()\n",
    "\n",
    "print(type(TOCHNN).__name__)\n",
    "print(algorithm_class_score(TOCHNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# The higher the score, the smaller the percentage of features with hight coorelation in relation to the average coorelation \n",
    "def correlated_features_score(train_data, test_data, thresholds=[0.05, 0.16, 0.28, 0.4], target_column=None, verbose=False):\n",
    "    \n",
    "    test_data = test_data.copy()\n",
    "    train_data = train_data.copy()\n",
    "     \n",
    "    if target_column:\n",
    "        X_test = test_data.drop(target_column, axis=1)\n",
    "        X_train = train_data.drop(target_column, axis=1)\n",
    "    else:\n",
    "        X_test = test_data.iloc[:,:-1]\n",
    "        X_train = train_data.iloc[:,:-1]\n",
    "        \n",
    "    \n",
    "    df_comb = pd.concat([X_test, X_train])\n",
    "    df_comb = df_comb._get_numeric_data()\n",
    "    corr_matrix = df_comb.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    \n",
    "    # Compute average and standar deviation from upper correlation matrix \n",
    "    avg_corr = upper.values[np.triu_indices_from(upper.values,1)].mean()\n",
    "    std_corr = upper.values[np.triu_indices_from(upper.values,1)].std()\n",
    "\n",
    "    # Find features with correlation greater than avg_corr + std_corr\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > (avg_corr+std_corr))]\n",
    "    if verbose: print(f\"Removed features: {to_drop}\")\n",
    "    \n",
    "    pct_drop = len(to_drop)/len(df_comb.columns)\n",
    "    \n",
    "    bins = thresholds\n",
    "    score = 5-np.digitize(pct_drop, bins, right=True) \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experimental purposes there will be used the following datasets:\n",
    "\n",
    "- [Healthcare Diabetes Dataset](https://www.kaggle.com/datasets/nanditapore/healthcare-diabetes)\n",
    "- [Iris Dataset](https://www.kaggle.com/datasets/uciml/iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed features: ['Insulin', 'BMI']\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_9748\\3473009596.py:23: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    }
   ],
   "source": [
    "# Example with Healthcare Diabetes Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "health = pd.read_csv('Data/Healthcare-Diabetes.csv')\n",
    "\n",
    "health_X = health[health.columns[1:9]]\n",
    "health_y = health[health.columns[-1]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(health_X, health_y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(correlated_features_score(X_train, X_test, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed features: ['petallength']\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_9748\\3473009596.py:23: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    }
   ],
   "source": [
    "# Example with Iris Dataset\n",
    "iris = pd.read_csv('Data/iris.csv')\n",
    "\n",
    "iris_X = iris[iris.columns[:5]]\n",
    "iris_y = iris['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.33, random_state=42)\n",
    "print(correlated_features_score(X_train, X_test, verbose=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Size Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Returns a score based on the number of attributes(columns) in the dataset\n",
    "def model_size_score(test_data, thresholds = np.array([10,30,100,500])):\n",
    "    print(test_data.shape)\n",
    "    dist_score = 5- np.digitize(test_data.shape[1]-1 , thresholds, right=True) # -1 for the id?\n",
    "    \n",
    "    return dist_score\n",
    "\n",
    "print(model_size_score(iris_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Relevance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'close_matches = get_close_matches(clf_name, alg_score.keys(), n=1, cutoff=0.6)\\n    if close_matches:\\n        exp_score = alg_score[close_matches[0]]\\n        return exp_score'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_relevance_score(clf):\n",
    "\n",
    "    scale_factor = 1.5 \n",
    "    distri_threshold = 0.5\n",
    "\n",
    "    # Feature Importance for Regressions \n",
    "    if (type(clf).__name__ == 'LogisticRegression') or (type(clf).__name__ == 'LinearRegression'): \n",
    "        importance = clf.coef_.flatten()\n",
    "\n",
    "        total = 0\n",
    "        for i in range(len(importance)):\n",
    "            total += abs(importance[i])\n",
    "\n",
    "        for i in range(len(importance)):\n",
    "            importance[i] = abs(importance[i]) / total\n",
    "\n",
    "    # Feature Importance fo Random Forest, model needs to be fitted\n",
    "    elif  (type(clf).__name__ == 'RandomForestClassifier') or (type(clf).__name__ == 'DecisionTreeClassifier'):\n",
    "        importance = clf.feature_importances_\n",
    "   \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "    # absolut values\n",
    "    importance = importance\n",
    "    indices = np.argsort(importance)[::-1] # indice of the biggest value in the importance list\n",
    "    importance = importance[indices]\n",
    "    \n",
    "    # calculate quantiles for outlier detection\n",
    "    q1, q3 = np.percentile(importance, [25,75])\n",
    "    lower_threshold , upper_threshold = q1 - scale_factor*(q3-q1),  q3 + scale_factor*(q3-q1) \n",
    "    \n",
    "    # percentage of features that concentrate distri_threshold percent of all importance\n",
    "    pct_dist = sum(np.cumsum(importance) < distri_threshold) / len(importance)\n",
    "    \n",
    "    return pct_dist\n",
    "    \n",
    "\n",
    "'''close_matches = get_close_matches(clf_name, alg_score.keys(), n=1, cutoff=0.6)\n",
    "    if close_matches:\n",
    "        exp_score = alg_score[close_matches[0]]\n",
    "        return exp_score'''\n",
    "# add flexibility in the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.2\n",
      "0.2\n",
      "0.3\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for i in range(2,11,2):\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_informative=i, n_redundant=0, n_repeated=0, n_clusters_per_class=2, n_classes=2, random_state=42)\n",
    "    clf = RandomForestClassifier(random_state=123)\n",
    "    clf.fit(X,y)\n",
    "\n",
    "    print(feature_relevance_score(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1\n",
      "0.1\n",
      "0.2\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for i in range(2,11,2):\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_informative=i, n_redundant=0, n_repeated=0, n_clusters_per_class=2, n_classes=2, random_state=42)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X,y)\n",
    "    \n",
    "    print(feature_relevance_score(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1\n",
      "0.1\n",
      "0.2\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "for i in range(2,11,2):\n",
    "    X, y = make_regression(n_samples=1000, n_features=10, n_informative=i, n_targets=1, random_state=123)\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X,y)\n",
    "    \n",
    "    print(feature_relevance_score(clf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_cv(test_sample, model, cfg):\n",
    "    \"\"\"Calculates feature importance coefficient of variation\n",
    "       :param test_sample: one test sample\n",
    "       :param model: the model\n",
    "       :param cfg: configs\n",
    "       :return: the coefficient of variation of the feature importance scores, [0, 1]\n",
    "    \"\"\"\n",
    "    cv = 0\n",
    "    batch_size = cfg['batch_size']\n",
    "    device = cfg['device']\n",
    "    if isinstance(model, torch.nn.Module):\n",
    "        batched_data, _ = test_sample\n",
    "\n",
    "        n = batch_size\n",
    "        m = math.floor(0.8 * n)\n",
    "\n",
    "        background = batched_data[:m].to(device)\n",
    "        test_data = batched_data[m:n].to(device)\n",
    "\n",
    "        e = shap.DeepExplainer(model, background)\n",
    "        shap_values = e.shap_values(test_data)\n",
    "        if shap_values is not None and len(shap_values) > 0:\n",
    "            sums = np.array([shap_values[i].sum() for i in range(len(shap_values))])\n",
    "            abs_sums = np.absolute(sums)\n",
    "            cv = variation(abs_sums)\n",
    "    return cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
