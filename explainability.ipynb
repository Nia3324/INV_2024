{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Class Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "\n",
    "def algorithm_class_score(model):\n",
    "    \"\"\"Returns an explainability score based on the model class. More complex models, will have a lower score.\n",
    "        :param model: model to be tested\n",
    "        :return: normalized score of [1, 5]\n",
    "    \"\"\"\n",
    "\n",
    "    alg_score = {\n",
    "    \"RandomForestClassifier\": 4,\n",
    "    \"KNeighborsClassifier\": 3,\n",
    "    \"SVC\": 2,\n",
    "    \"GaussianProcessClassifier\": 3,\n",
    "    \"DecisionTreeClassifier\": 5,\n",
    "    \"MLPClassifier\": 1,\n",
    "    \"AdaBoostClassifier\": 3,\n",
    "    \"GaussianNB\": 3.5,\n",
    "    \"QuadraticDiscriminantAnalysis\": 3,\n",
    "    \"LogisticRegression\": 4,\n",
    "    \"LinearRegression\": 3.5,\n",
    "    }\n",
    "\n",
    "    model_name = type(model).__name__\n",
    "\n",
    "    # Check if the model_name is in the dictionary\n",
    "    if model_name in alg_score:\n",
    "        exp_score = alg_score[model_name]\n",
    "        return exp_score \n",
    "\n",
    "    # Check if the model is a Neural Network\n",
    "    if isinstance(model, tf.keras.Model) or isinstance(model, tf.Module) or isinstance(model, nn.Module):\n",
    "        return 1\n",
    "    \n",
    "    # If not, try to find a close match\n",
    "    close_matches = get_close_matches(model_name, alg_score.keys(), n=1, cutoff=0.6)\n",
    "    if close_matches:\n",
    "        exp_score = alg_score[close_matches[0]]\n",
    "        return exp_score\n",
    "    \n",
    "    # If no close match found \n",
    "    print(f\"No matching score found for '{model_name}'\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "5\n",
      "DecisionTreeRegressor\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Example Decision Tree Classifer and Regressor\n",
    "from sklearn import tree\n",
    "\n",
    "Classifier = tree.DecisionTreeClassifier()\n",
    "Regressor = tree.DecisionTreeRegressor()\n",
    "\n",
    "print(type(Classifier).__name__)\n",
    "print(algorithm_class_score(Classifier))\n",
    "\n",
    "print(type(Regressor).__name__)\n",
    "print(algorithm_class_score(Regressor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Example Neural Network Tensorflow \n",
    "import tensorflow as tf\n",
    "\n",
    "TFNN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, input_dim=128, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(type(TFNN).__name__)\n",
    "print(algorithm_class_score(TFNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Custom non-sequential NN using keras\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.dense2(x)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "print(type(model).__name__)\n",
    "print(algorithm_class_score(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Example Neural Network Pytoch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 64)  \n",
    "        self.fc2 = nn.Linear(64, 32)  \n",
    "        self.fc3 = nn.Linear(32, 1)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))   \n",
    "        x = torch.relu(self.fc2(x))    \n",
    "        x = torch.sigmoid(self.fc3(x)) \n",
    "        return x\n",
    "    \n",
    "TOCHNN = NeuralNetwork()\n",
    "\n",
    "print(type(TOCHNN).__name__)\n",
    "print(algorithm_class_score(TOCHNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# The higher the score, the smaller the percentage of features with hight coorelation in relation to the average coorelation \n",
    "def correlated_features_score(train_data, test_data, thresholds=[0.05, 0.16, 0.28, 0.4], target_column=None, verbose=False):\n",
    "    print(type(test_data))\n",
    "    if type(test_data) != 'pandas.core.frame.DataFrame':\n",
    "        test_data = pd.DataFrame(test_data)\n",
    "\n",
    "    test_data = test_data.copy()\n",
    "    train_data = train_data.copy()\n",
    "     \n",
    "    if target_column:\n",
    "        X_test = test_data.drop(target_column, axis=1)\n",
    "        X_train = train_data.drop(target_column, axis=1)\n",
    "    else:\n",
    "        X_test = test_data.iloc[:,:-1]\n",
    "        X_train = train_data.iloc[:,:-1]\n",
    "        \n",
    "    \n",
    "    df_comb = pd.concat([X_test, X_train])\n",
    "    df_comb = df_comb._get_numeric_data()\n",
    "    corr_matrix = df_comb.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    \n",
    "    # Compute average and standar deviation from upper correlation matrix \n",
    "    avg_corr = upper.values[np.triu_indices_from(upper.values,1)].mean()\n",
    "    std_corr = upper.values[np.triu_indices_from(upper.values,1)].std()\n",
    "\n",
    "    # Find features with correlation greater than avg_corr + std_corr\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > (avg_corr+std_corr))]\n",
    "    if verbose: print(f\"Removed features: {to_drop}\")\n",
    "    \n",
    "    pct_drop = len(to_drop)/len(df_comb.columns)\n",
    "    \n",
    "    bins = thresholds\n",
    "    score = 5-np.digitize(pct_drop, bins, right=True) \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experimental purposes there will be used the following datasets:\n",
    "\n",
    "- [Healthcare Diabetes Dataset](https://www.kaggle.com/datasets/nanditapore/healthcare-diabetes)\n",
    "- [Iris Dataset](https://www.kaggle.com/datasets/uciml/iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Removed features: ['Insulin', 'BMI']\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_2716\\191569567.py:26: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    }
   ],
   "source": [
    "# Example with Healthcare Diabetes Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "health = pd.read_csv('Data/Healthcare-Diabetes.csv')\n",
    "\n",
    "health_X = health[health.columns[1:9]]\n",
    "health_y = health[health.columns[-1]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(health_X, health_y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(correlated_features_score(X_train, X_test, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed features: ['petallength']\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_9748\\3473009596.py:23: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    }
   ],
   "source": [
    "# Example with Iris Dataset\n",
    "iris = pd.read_csv('Data/iris.csv')\n",
    "\n",
    "iris_X = iris[iris.columns[:5]]\n",
    "iris_y = iris['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.33, random_state=42)\n",
    "print(correlated_features_score(X_train, X_test, verbose=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Size Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Returns a score based on the number of attributes(columns) in the dataset\n",
    "def model_size_score(test_data, thresholds = np.array([10,30,100,500])):\n",
    "    print(test_data.shape)\n",
    "    dist_score = 5- np.digitize(test_data.shape[1]-1 , thresholds, right=True) # -1 for the id?\n",
    "    \n",
    "    return dist_score\n",
    "\n",
    "print(model_size_score(iris_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size(model, test_dataset=None):\n",
    "    \"\"\"\n",
    "    Calculate the size of a machine learning model based on its type.\n",
    "    \n",
    "    Parameters:\n",
    "    model : object\n",
    "        The machine learning model whose size needs to be determined.\n",
    "    test_dataset : array-like, optional\n",
    "        A test dataset to be used for certain types of models.\n",
    "\n",
    "    Returns:\n",
    "    int or str or None\n",
    "        The size of the model, which can be the number of parameters, \n",
    "        the number of nodes, the number of support vectors, or the \n",
    "        number of features seen in fit, depending on the model type.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the model is a TensorFlow Keras Model, return the count of parameters\n",
    "    if isinstance(model, tf.keras.Model):\n",
    "        return model.count_params()\n",
    "    \n",
    "    # If the model is a TensorFlow Module or PyTorch Module, return the sum of parameters\n",
    "    elif isinstance(model, tf.Module) or isinstance(model, nn.Module):\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # If the model has 'estimators_', typically an ensemble model like RandomForest\n",
    "    elif hasattr(model, 'estimators_'):\n",
    "        count = 0\n",
    "        for i, est in enumerate(model.estimators_):\n",
    "            # If the estimator has a tree structure, add the number of nodes\n",
    "            if hasattr(est, 'tree_'):\n",
    "                count += est.tree_.node_count\n",
    "            # If the estimator has support vectors, add their count\n",
    "            elif hasattr(est, 'n_support_'):\n",
    "                count += sum(est.n_support_)\n",
    "        return count\n",
    "    \n",
    "    # If the model is a Support Vector Classifier\n",
    "    elif hasattr(model, 'SVC'):\n",
    "        return sum(model.n_support_)\n",
    "\n",
    "    # If the model has a tree structure, return the number of nodes\n",
    "    elif hasattr(model, 'tree_'):\n",
    "        return model.tree_.node_count\n",
    "    \n",
    "    # Return the number of features seen during fit if applicable\n",
    "    elif hasattr(model, 'n_features_in_'):\n",
    "        return 'n_features_in_'\n",
    "    \n",
    "    # If a test dataset is provided, return the number of features in the dataset\n",
    "    elif test_dataset is not None:\n",
    "        return test_dataset.shape[1]\n",
    "    \n",
    "    # If none of the above conditions are met, return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X, y)\n",
    "\n",
    "print(model_size(rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load a sample dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train an AdaBoost Classifier with default base estimator (decision stumps)\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X, y)\n",
    "\n",
    "print(model_size(ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374\n",
      "False\n",
      "[20 19]\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "clf = BaggingClassifier(estimator=SVC(),\n",
    "                        n_estimators=10, random_state=0).fit(X, y)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(clf.estimators_)):\n",
    "    if hasattr(clf.estimators_[i], 'n_support_'):\n",
    "        count += sum(clf.estimators_[i].n_support_)\n",
    "print(count)\n",
    "\n",
    "print(hasattr(clf.estimators_[1], 'n_support'))\n",
    "\n",
    "print(clf.estimators_[1].n_support_)\n",
    "\n",
    "print(model_size(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8635 - loss: 0.4705 - val_accuracy: 0.9523 - val_loss: 0.1640\n",
      "Epoch 2/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9598 - loss: 0.1393 - val_accuracy: 0.9672 - val_loss: 0.1122\n",
      "Epoch 3/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9734 - loss: 0.0870 - val_accuracy: 0.9720 - val_loss: 0.0974\n",
      "Epoch 4/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9825 - loss: 0.0605 - val_accuracy: 0.9712 - val_loss: 0.0985\n",
      "Epoch 5/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9867 - loss: 0.0455 - val_accuracy: 0.9748 - val_loss: 0.0878\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9733 - loss: 0.0883\n",
      "Test accuracy: 0.9765999913215637\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1] range\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Step 2: Create a simple neural network model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the input image\n",
    "    Dense(128, activation='relu'),  # First hidden layer\n",
    "    Dense(10, activation='softmax') # Output layer\n",
    "])\n",
    "\n",
    "# Step 3: Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101770"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "dt.fit(X, y)\n",
    "\n",
    "print(model_size(dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Relevance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "\n",
    "def feature_relevance_score(model, thresholds = [0.05, 0.1, 0.2, 0.3]):\n",
    "\n",
    "    scale_factor = 1.5 \n",
    "    distri_threshold = 0.5\n",
    "\n",
    "    regression = ['LogisticRegression', 'LogisticRegression']\n",
    "    classifier = ['RandomForestClassifier', 'DecisionTreeClassifier']\n",
    "\n",
    "    # Feature Importance for Regressions \n",
    "    if (type(model).__name__ in regression) or (get_close_matches(type(model).__name__, regression, n=1, cutoff=0.6)): \n",
    "        importance = model.coef_.flatten()\n",
    "\n",
    "        total = 0\n",
    "        for i in range(len(importance)):\n",
    "            total += abs(importance[i])\n",
    "\n",
    "        for i in range(len(importance)):\n",
    "            importance[i] = abs(importance[i]) / total\n",
    "\n",
    "    # Feature Importance fo Random Forest, model needs to be fitted\n",
    "    elif  (type(model).__name__ in classifier) or (get_close_matches(type(model).__name__, classifier, n=1, cutoff=0.6)):\n",
    "        importance = model.feature_importances_\n",
    "   \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # absolut values\n",
    "    importance = importance\n",
    "    indices = np.argsort(importance)[::-1] # indice of the biggest value in the importance list\n",
    "    importance = importance[indices]\n",
    "    \n",
    "    # calculate quantiles for outlier detection\n",
    "    q1, q3 = np.percentile(importance, [25,75])\n",
    "    lower_threshold , upper_threshold = q1 - scale_factor*(q3-q1),  q3 + scale_factor*(q3-q1) \n",
    "    \n",
    "    # percentage of features that concentrate distri_threshold percent of all importance\n",
    "    pct_dist = sum(np.cumsum(importance) < distri_threshold) / len(importance)\n",
    "    \n",
    "    score = np.digitize(pct_dist, thresholds, right=False) + 1 \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for i in range(2,11,2):\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_informative=i, n_redundant=0, n_repeated=0, n_clusters_per_class=2, n_classes=2, random_state=42)\n",
    "    model = RandomForestClassifier(random_state=123)\n",
    "    model.fit(X,y)\n",
    "\n",
    "    print(feature_relevance_score(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1\n",
      "0.1\n",
      "0.2\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for i in range(2,11,2):\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_informative=i, n_redundant=0, n_repeated=0, n_clusters_per_class=2, n_classes=2, random_state=42)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X,y)\n",
    "    \n",
    "    print(feature_relevance_score(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1\n",
      "0.1\n",
      "0.2\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "for i in range(2,11,2):\n",
    "    X, y = make_regression(n_samples=1000, n_features=10, n_informative=i, n_targets=1, random_state=123)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X,y)\n",
    "    \n",
    "    print(feature_relevance_score(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import math\n",
    "from scipy.stats import variation\n",
    "\n",
    "def get_feature_importance_cv(test_sample, model, cfg):\n",
    "    \"\"\"Calculates feature importance coefficient of variation\n",
    "       :param test_sample: one test sample\n",
    "       :param model: the model\n",
    "       :param cfg: configs\n",
    "       :return: the coefficient of variation of the feature importance scores, [0, 1]\n",
    "    \"\"\"\n",
    "    cv = 0\n",
    "    batch_size = cfg['batch_size']\n",
    "    device = cfg['device']\n",
    "    if isinstance(model, torch.nn.Module):\n",
    "        batched_data, _ = test_sample\n",
    "\n",
    "        n = batch_size\n",
    "        m = math.floor(0.8 * n)\n",
    "\n",
    "        background = batched_data[:m].to(device)\n",
    "        test_data = batched_data[m:n].to(device)\n",
    "\n",
    "        e = shap.DeepExplainer(model, background)\n",
    "        shap_values = e.shap_values(test_data)\n",
    "        if shap_values is not None and len(shap_values) > 0:\n",
    "            sums = np.array([shap_values[i].sum() for i in range(len(shap_values))])\n",
    "            abs_sums = np.absolute(sums)\n",
    "            cv = variation(abs_sums)\n",
    "    return cv\n",
    "\n",
    "\n",
    "\n",
    "# -> agnostic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\anaconda3\\envs\\INV\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.            0.          -26.3898406     0.            0.\n",
      "    0.         -173.96069793  -36.47846048  178.36556706    0.        ]\n",
      "[  0.           0.          26.3898406    0.           0.\n",
      "   0.         173.96069793  36.47846048 178.36556706   0.        ]\n",
      "1.6488185941212234\n",
      "[   0.            7.95771464   -7.50706771    0.            0.\n",
      "    0.           32.69152363    4.95209513  -20.46210449 -112.74815127]\n",
      "[  0.           7.95771464   7.50706771   0.           0.\n",
      "   0.          32.69152363   4.95209513  20.46210449 112.74815127]\n",
      "1.7697208040239418\n",
      "[ 139.29544059   15.62943679   10.79985054    0.          107.32512664\n",
      "    0.          -76.94435297    4.72924418 -106.35838137  160.60253257]\n",
      "[139.29544059  15.62943679  10.79985054   0.         107.32512664\n",
      "   0.          76.94435297   4.72924418 106.35838137 160.60253257]\n",
      "0.9608445625198315\n",
      "[  41.69635121 -143.00262003   25.77577464  -30.38333323   27.94207704\n",
      "   28.00904702  -15.18115511   -0.86557109   90.38688799   17.81065737]\n",
      "[ 41.69635121 143.00262003  25.77577464  30.38333323  27.94207704\n",
      "  28.00904702  15.18115511   0.86557109  90.38688799  17.81065737]\n",
      "0.9597673377706508\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import variation\n",
    "import shap\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming model is your trained model\n",
    "# X_train is your training dataset\n",
    "# X_explain is the dataset you want to explain\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(model): \n",
    "    return model.predict_proba if hasattr(model, 'predict_proba') else model.predict\n",
    "\n",
    "\n",
    "def feature_importance(model, test_data):\n",
    "    # Create a background dataset (subset of your training data)\n",
    "    background = shap.sample(test_data, 100)  # Using 100 samples for background\n",
    "\n",
    "    # Initialize KernelExplainer with the prediction function and background dataset\n",
    "    explainer = shap.KernelExplainer(predict(model), background)\n",
    "\n",
    "    # Calculate SHAP values for the dataset you want to explain\n",
    "    shap_values = explainer.shap_values(X.iloc[0])\n",
    "  \n",
    "    # calculare variance of absolute shap values \n",
    "    if shap_values is not None and len(shap_values) > 0:\n",
    "        sums = np.array([abs(shap_values[i]).sum() for i in range(len(shap_values))])\n",
    "        print(sums)\n",
    "        cv = np.std(sums) / np.mean(sums)\n",
    "        return cv\n",
    "\n",
    "\n",
    "for i in range(4,11,2):\n",
    "    X, y = make_regression(n_samples=1000, n_features=10, n_informative=i, n_targets=1, random_state=123)\n",
    "    model = LinearRegression()\n",
    "\n",
    "    #X, y = make_classification(n_samples=1000, n_features=10, n_informative=i,flip_y=0, n_redundant=0, n_repeated=0, n_clusters_per_class=2, n_classes=5, random_state=42)\n",
    "    #model = RandomForestClassifier(random_state=123)\n",
    "    \n",
    "    model.fit(X,y)\n",
    "    X = pd.DataFrame(X)\n",
    "    print(feature_importance(model, X))\n",
    "\n",
    "#deep modelssss and regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Custom non-sequential NN using keras\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.dense2(x)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "print(type(model).__name__)\n",
    "print(algorithm_class_score(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size_score(model, dataset):\n",
    "    # add comment\n",
    "    if isinstance(model, tf.keras.Model):\n",
    "        return (model.count_params())\n",
    "    elif isinstance(model, tf.Module) or isinstance(model, nn.Module):\n",
    "        return (sum(p.numel() for p in model.parameters()))\n",
    "    else:\n",
    "        return (dataset.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "from sklearn2pmml.util import deep_sizeof\n",
    "\n",
    "print(model.__sizeof__())\n",
    "#print(deep_sizeof(model, with_overhead=True, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 32 B\n",
      "Final fitted state: 32 B\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X, y = make_regression(n_samples = 10000, n_features = 10)\n",
    "\n",
    "estimator = RandomForestRegressor(n_estimators = 31, random_state = 13)\n",
    "estimator.fit(X, y)\n",
    "\n",
    "print(\"Initial state: {} B\".format(estimator.__sizeof__()))\n",
    "\n",
    "estimator.fit(X, y)\n",
    "\n",
    "print(\"Final fitted state: {} B\".format(estimator.__sizeof__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import types\n",
    "\n",
    "def is_instance_attr(obj, name):\n",
    "  if not hasattr(obj, name):\n",
    "    return False\n",
    "  if name.startswith(\"__\") and name.endswith(\"__\"):\n",
    "    return False\n",
    "  v = getattr(obj, name)\n",
    "  if isinstance(v, (types.BuiltinFunctionType, types.BuiltinMethodType, types.FunctionType, types.MethodType)):\n",
    "    return False\n",
    "  # See https://stackoverflow.com/a/17735709/\n",
    "  attr_type = getattr(type(obj), name, None)\n",
    "  if isinstance(attr_type, property):\n",
    "    return False\n",
    "  return True\n",
    "\n",
    "def get_instance_attrs(obj):\n",
    "  names = dir(obj)\n",
    "  names = [name for name in names if is_instance_attr(obj, name)]\n",
    "  return names\n",
    "\n",
    "def deep_sklearn_sizeof(obj, verbose = True):\n",
    "  # Primitive type values\n",
    "  if obj is None:\n",
    "    return obj.__sizeof__()\n",
    "  elif isinstance(obj, (int, float, str, bool, numpy.int64, numpy.float32, numpy.float64)):\n",
    "    return obj.__sizeof__()\n",
    "  # Iterables\n",
    "  elif isinstance(obj, list):\n",
    "    sum = [].__sizeof__() # Empty list\n",
    "    for v in obj:\n",
    "      v_sizeof = deep_sklearn_sizeof(v, verbose = False)\n",
    "      sum += v_sizeof\n",
    "    return sum\n",
    "  elif isinstance(obj, tuple):\n",
    "    sum = ().__sizeof__() # Empty tuple\n",
    "    for i, v in enumerate(obj):\n",
    "      v_sizeof = deep_sklearn_sizeof(v, verbose = False)\n",
    "      sum += v_sizeof\n",
    "    return sum\n",
    "  # Numpy ndarrays\n",
    "  elif isinstance(obj, numpy.ndarray):\n",
    "    sum = obj.__sizeof__() # Array header\n",
    "    sum += (obj.size * obj.itemsize) # Array content\n",
    "    return sum\n",
    "  # Reference type values\n",
    "  else:\n",
    "    clazz = obj.__class__\n",
    "    qualname = \".\".join([clazz.__module__, clazz.__name__])\n",
    "    \n",
    "    # Restrict the circle of competence to Scikit-Learn classes\n",
    "    if not (qualname.startswith(\"_abc.\") or qualname.startswith(\"sklearn.\")):\n",
    "      raise ValueError(qualname)\n",
    "    \n",
    "    sum = object().__sizeof__() # Empty object\n",
    "    names = get_instance_attrs(obj)\n",
    "    if names:\n",
    "      if verbose:\n",
    "        print(\"| Attribute | `type(v)` | `deep_sklearn_sizeof(v)` |\")\n",
    "        print(\"|---|---|---|\")\n",
    "      for name in names:\n",
    "        v = getattr(obj, name)\n",
    "        v_type = type(v)\n",
    "        v_sizeof = deep_sklearn_sizeof(v, verbose = False)\n",
    "        sum += v_sizeof\n",
    "        if verbose:\n",
    "          print(\"| {} | {} | {} |\".format(name, v_type, v_sizeof))\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Attribute | `type(v)` | `deep_sklearn_sizeof(v)` |\n",
      "|---|---|---|\n",
      "| _abc_impl | <class '_abc._abc_data'> | 16 |\n",
      "| _estimator_type | <class 'str'> | 58 |\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "builtins.dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m clf \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m      5\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X,y)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdeep_sklearn_sizeof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[12], line 63\u001b[0m, in \u001b[0;36mdeep_sklearn_sizeof\u001b[1;34m(obj, verbose)\u001b[0m\n\u001b[0;32m     61\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, name)\n\u001b[0;32m     62\u001b[0m v_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(v)\n\u001b[1;32m---> 63\u001b[0m v_sizeof \u001b[38;5;241m=\u001b[39m \u001b[43mdeep_sklearn_sizeof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m v_sizeof\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[1;32mIn[12], line 53\u001b[0m, in \u001b[0;36mdeep_sklearn_sizeof\u001b[1;34m(obj, verbose)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Restrict the circle of competence to Scikit-Learn classes\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (qualname\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_abc.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m qualname\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn.\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m---> 53\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(qualname)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m()\u001b[38;5;241m.\u001b[39m__sizeof__() \u001b[38;5;66;03m# Empty object\u001b[39;00m\n\u001b[0;32m     55\u001b[0m names \u001b[38;5;241m=\u001b[39m get_instance_attrs(obj)\n",
      "\u001b[1;31mValueError\u001b[0m: builtins.dict"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=4, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "\n",
    "\n",
    "print(deep_sklearn_sizeof(model, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "\n",
    "\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))\n",
    "\n",
    "\n",
    "model size: 44.629MB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
